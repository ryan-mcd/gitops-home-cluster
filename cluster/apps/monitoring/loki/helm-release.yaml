---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      chart: loki
      version: 5.19.0
      sourceRef:
        kind: HelmRepository
        name: grafana-charts
        namespace: flux-system
      interval: 5m
  values:
    ingress:
      enabled: true
      # ingressClassName: "traefik"
      annotations:
        kubernetes.io/ingress.class: "traefik"
        traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
        traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918@kubernetescrd"
        cert-manager.io/cluster-issuer: "letsencrypt-production"
      hosts:
        - host: "loki.${DOMAIN_2}"
          paths:
            - /
      tls:
        - hosts:
            - "loki.${DOMAIN_2}"
          secretName: "loki-${DOMAIN_2/./-}-tls"
    serviceMonitor:
      enabled: true
    persistence:
      enabled: false
    config:
      storage_config:
        aws:
          bucketnames: loki
          endpoint: ${MINIO_ADDR}:9768
          access_key_id: "${SECRET_MINIO_ACCESS_KEY}"
          secret_access_key: "${SECRET_MINIO_SECRET_KEY}"
          s3forcepathstyle: true
          insecure: true
        boltdb_shipper:
          active_index_directory: /data/loki/index
          cache_location: /data/loki/index_cache
          resync_interval: 5s
          shared_store: s3
      # Needed for Alerting: https://grafana.com/docs/loki/latest/alerting/
      # This is just a simple example, for more details: https://grafana.com/docs/loki/latest/configuration/#ruler_config
      ruler:
        storage:
          type: local
          local:
            directory: /rules
        rule_path: /tmp/scratch
        alertmanager_url: http://prometheus-alertmanager:9093
        ring:
          kvstore:
            store: inmemory
        enable_api: true
    alerting_groups:
      #
      # SMART Failures
      #
      - name: smart-failure
        rules:
          - alert: SmartFailures
            expr: |
              sum by (hostname) (count_over_time({hostname=~".+"} | json | _SYSTEMD_UNIT = "smartmontools.service" !~ "(?i)previous self-test completed without error" !~ "(?i)Prefailure" |~ "(?i)(error|fail)"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "SMART has reported failures on host {{ $labels.hostname }}"
      #
      # home-assistant
      #
      - name: home-assistant
        rules:
          - alert: HomeAssistantUnableToReachPostgresql
            expr: |
              sum by (app) (count_over_time({app="home-assistant"} |~ "(?i)error in database connectivity"[2m])) > 0
            for: 2m
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Home Assistant is unable to connect to postgresql"
      # #
      # # node-red
      # #
      # - name: node-red
      #   rules:
      #     - alert: NodeRedUnableToReachHomeAssistant
      #       expr: |
      #         sum by (app) (count_over_time({app="node-red"} |~ "(?i)home assistant.*connecting to undefined"[2m])) > 0
      #       for: 2m
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "Node-Red is unable to connect to Home Assistant"
      # #
      # # frigate
      # #
      # - name: frigate
      #   rules:
      #     - alert: FrigateUnableToReachMQTT
      #       expr: |
      #         sum(count_over_time({app="frigate"} |~ "(?i)unable to connect to mqtt server"[2m])) > 0
      #       for: 2m
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "Frigate is unable to reach MQTT"
      # #
      # # zigbee2mqtt
      # #
      # - name: zigbee2mqtt
      #   rules:
      #     - alert: ZigbeeUnableToReachMQTT
      #       expr: |
      #         sum(count_over_time({app="zigbee2mqtt"} |~ "(?i)not connected to mqtt server"[2m])) > 0
      #       for: 2m
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "Zigbee2mqtt is unable to reach MQTT"
      # #
      # # zwavejs2mqtt
      # #
      # - name: zwavejs2mqtt
      #   rules:
      #     - alert: ZwaveUnableToReachMQTT
      #       expr: |
      #         sum(count_over_time({app="zwavejs2mqtt"} |~ "(?i)error while connecting mqtt"[2m])) > 0
      #       for: 2m
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "Zwavejs2mqtt is unable to reach MQTT"
      # #
      # # valetudo
      # #
      # - name: valetudo
      #   rules:
      #     - alert: ValetudoUnableToReachMQTT
      #       expr: |
      #         sum by (hostname) (count_over_time({hostname="valetudo"} |~ "(?i).*error.*mqtt.*"[2m])) > 0
      #       for: 2m
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "Valetudo is unable to connect to mqtt"
    tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"